{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAFA Multiclass Classification Tutorial\n",
    "\n",
    "In this short tutorial, I want to guide you in implementing a minimal machine learning pipeline to learn to predict gene ontology (GO) terms from amino acid sequences (primary structures of proteins). This is also called the Critical Assessment of Function Annotation (CAFA) challenge.\n",
    "\n",
    "This is only meant as a minimal template upon which you can build your own pipeline for the CAFA challenge, though. This does not make use of the GO graph nor other annotation data (taxonomy, etc.) yet!\n",
    "\n",
    "The data used in this tutorial is taken from the CAFA 6 competition on Kaggle: https://www.kaggle.com/competitions/cafa-6-protein-function-prediction/data\n",
    "\n",
    "Since this data is only available to users that have agreed to the competition's terms, I don't think I can host them publicly. However, you can easily register an account for yourself on Kaggle (free), participate in the competition (free), and download the data yourself (also free).\n",
    "\n",
    "## Target Audience\n",
    "\n",
    "This is a tutorial aimed towards beginners in machine learning by a beginner in machine learning himself. I want to write something that resonates with people who are not good at maths nor fond of textbooks but like hands-on learning.\n",
    "\n",
    "You do need to know how to use Jupyter notebooks and a bit of Python and Pandas, though. I use idiomatic Python and Pandas operations in this tutorial, which may look arcane if you are not familiar (or you learned bad coding habits :P).\n",
    "\n",
    "## Setup\n",
    "\n",
    "You can install Python and dependencies however you want, but I provided a standard `uv` project:\n",
    "\n",
    "- Install `uv`\n",
    "- `uv venv`\n",
    "- `uv lock`\n",
    "- `source .venv`\n",
    "- `jupyter lab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from pyfaidx import Fasta\n",
    "from torch.nn.functional import one_hot\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train data, assuming you have downloaded them\n",
    "# Here, I assumed you downloaded them into the same directory/folder as this notebook file\n",
    "# However, if that is not the case, just modify the file paths accordingly\n",
    "\n",
    "train_terms = pd.read_csv(\"train_terms.tsv\", sep='\\t')\n",
    "seq = Fasta(\"train_sequences.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to only use a small subnet of the data if needed (to quickly test things, etc.)\n",
    "# Adjust the frac parameter to set the percentage of data you want\n",
    "\n",
    "train_terms = train_terms.sample(frac=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The entire point of this code cell is to encode the response/target variable (the GO terms) into a data format that ML algorithms can understand\n",
    "\n",
    "# Collect unique GO terms in our training data\n",
    "unique_terms = train_terms['term'].unique()\n",
    "\n",
    "# Map GO terms to numeric values (machine learning only works with numbers, so anything non-numeric has to be converted at some point) and vice versa\n",
    "id2label = {idx: term for idx, term in enumerate(unique_terms)}\n",
    "label2id = {term: idx for idx, term in enumerate(unique_terms)}\n",
    "\n",
    "# In the FASTA file, a sequence's key includes more than its EntryID in the train_terms.tsv file\n",
    "# So we want to build a mapping from EntryID to the raw sequence for quick lookup\n",
    "seqs = {seq[key].name.split('|')[1] : seq[key][:].seq for key in seq.keys()}\n",
    "\n",
    "# Add the proteins' sequences to the training data\n",
    "# These will be the feature/predictor/X that our model will learn to predict the target/response/y from\n",
    "train_terms['seq'] = train_terms['EntryID'].map(lambda x: seqs[x])\n",
    "\n",
    "# Since we want to learn multiple labels for each protein sequence, we need to associate each of them to a list of labels\n",
    "# We also need to encode that list into an array of real numbers in order to feed it into our ML model (as mentioned above, ML only works with numbers and multidimensional objects (arrays, matrices, tensors) of numbers)\n",
    "# In the original data, each protein-term pair is a single row, we want to \"collapse\" them into a single protein-(list of terms) row for each protein\n",
    "\n",
    "# Create a label column that is constructed as follow:\n",
    "# - For each value in the term column, find the integer assigned to it using our label2id mapping above\n",
    "# - Encode that value into a 1-hot array. Read more about one-hot encoding here:\n",
    "#   - https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html\n",
    "#   - https://en.wikipedia.org/wiki/One-hot\n",
    "train_terms['label'] = train_terms['term'].map(lambda x: one_hot(torch.tensor(label2id[x]), num_classes=unique_terms.size).numpy().astype(float))\n",
    "\n",
    "# \"Collapse\" the many labels for each protein by summing them up\n",
    "train_terms = train_terms.groupby('EntryID').agg({'label': 'sum', 'seq': 'first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might have learned that machine learning (and deep learning) is about learning high-dimensional features of data\n",
    "# Therefore, the very first step, conceptually, is \"bringing\" that data into a \"space\" that has enough dimensions to represent it\n",
    "# Practically, this means choosing a numerical space with enough dimensions and encode our data into elements in it\n",
    "# (because ML only works with- Ok, ok, I'll stop saying it)\n",
    "# In the latest lingo, this is called tokenization (I swear it was called embedding last time)\n",
    "# \n",
    "# For data that we have understood well, we can manually design rules to encode them\n",
    "# For example, if you only need to classify codons, then you may be able to hard-code arrays to represent them (since there are only a fixed amount of them)\n",
    "# But for data that we have not yet have a good understanding of, like proteins, we can also ask the machine to \"understand\" them for us (nothing can go wrong!)\n",
    "# Here, we use Facebook AI's Evolutionary Scale Modelling model, which was trained to do exactly that\n",
    "# (There are many other \"protein language\" models, but this one integrates well with the Transformers framework)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "def batch_tokenize(examples):\n",
    "    return tokenizer(examples['seq'])\n",
    "\n",
    "dataset = Dataset.from_pandas(train_terms[['seq', 'label']])\n",
    "dataset = dataset.map(batch_tokenize, batched=True)\n",
    "\n",
    "# After tokenizing, we split our dataset into a training set and a test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of these Tokenizer, AutoModel, TrainingArguments, and Trainer objects have a ton of parameters (see Transformers' documentation)\n",
    "# But these are the absolute minimum required to successfully run a multiclass classification trainer\n",
    "# It may perform terribly with these default params, but I want to give you a minimal template to work with\n",
    "# (Many tutorials provide initial values like learning rate, epoch, etc. without much explanation. I hate that)\n",
    "\n",
    "# We don't train a model from scratch, but only fine-tune Facebook's model for our data\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\", problem_type=\"multi_label_classification\", id2label=id2label, label2id=label2id\n",
    ")\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "\n",
    "    # These parameters are for displaying progress in the notebook\n",
    "    # Transformers (the library) has some questionable software design choices\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "\n",
    "    # This is for resizing all inputs (numerical representations of protein sequences) in a traning batch into the same size, usually the size of the biggest sequence of that batch\n",
    "    # Because...the algorithm requires it (to be most efficient)\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training (fine-tuning), the fine-tuned model will be saved in a \"checkpoint-*\" subdirectory of the \"./model\" directory\n",
    "# We will use that for predicting GO terms for a protein now\n",
    "# Edit the model parameter to point to the directory of the fine-tuned model\n",
    "# The classifier, when given an amino acid sequence, will return a dictionary of GO terms and the probabilities that each of those terms are associated with said sequence\n",
    "# The top_k parameter controls how many labels (GO terms) we want the classifier to return. \"None\" means all (that the model was fine-tuned on earlier)\n",
    "classifier = pipeline(task=\"text-classification\", model=\"./model/<checkpoint>\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('<some amino acid sequence>')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
